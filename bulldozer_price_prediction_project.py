# -*- coding: utf-8 -*-
"""Bulldozer Price Prediction Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Jl4p74Nti_Bg48-a8TDn8NupyQ7ki4js

# Predicting the sale price of bulldozers using Machine Learning

In this project, we will be going through the example and will predict the sale price of bulldozers.

## 1. Problem Definition
How well can we predict the future sale price of a bulldozer, given its characteristics and previous examples of how much similar bulldozers have been sold for?

## 2. Data
The data is downloaded from the Kaggle Bluebook for Bulldozers competition: https://www.kaggle.com/c/bluebook-for-bulldozers/data

There are 3 main datasets:

* Train.csv is the training set, which contains data through the end of 2011.
* Valid.csv is the validation set, which contains data from January 1, 2012 - April 30, 2012 You make predictions on this set throughout the majority of the competition. Your score on this set is used to create the public leaderboard.
* Test.csv is the test set, which won't be released until the last week of the competition. It contains data from May 1, 2012 - November 2012. Your score on the test set determines your final rank for the competition.

## 3. Evaluation
The evaluation metric for this competition is the RMSLE (root mean squared log error) between the actual and predicted auction prices.

For more on the evaluation of this project check: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation

Note: The goal for most regression evaluation metrics is to minimize the error. For example, our goal for this project will be to build a machine learning model which minimises RMSLE.

## 4. Features
Kaggle provides a data dictionary detailing all of the features of the dataset. You can view this data dictionary on Google Sheets: https://docs.google.com/spreadsheets/d/18ly-bLR8sbDJLITkWG7ozKm8l3RyieQ2Fpgix-beSYI/edit?usp=sharing
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import sklearn

# Import training and validation sets
df = pd.read_csv("/TrainAndValid.csv",
                 low_memory=False)

df.info()

df.isna().sum()

df.columns

fig, ax = plt.subplots()
ax.scatter(df["saledate"][:1000], df["SalePrice"][:1000])

df.SalePrice.plot.hist();

"""### Parsing Dates

When we work with time series data, we want to enrich the time and date component as much as possible.

We can do that by telling pandas which of our columns has dates in it using the `parse_dates` parameters.
"""

# Import data again but this time parse dates
df = pd.read_csv("/TrainAndValid.csv", low_memory=False, parse_dates=["saledate"])

df.saledate.dtype

df.saledate[:1000]

fig, ax = plt.subplots()
ax.scatter(df["saledate"][:1000], df["SalePrice"][:1000])

df.head()

# Transpose to view all the column names.
df.head().T

df.saledate.head(20)

"""### Sort DataFrame by saledate

When working with time series data, it's a good idea to sort it by date.
"""

# Sort DataFrame in date order
df.sort_values(by=["saledate"], inplace=True, ascending=True)
df.saledate.head(20)

df.head()

"""### Make a copy of the original DataFrame

We make a copy of the original DataFrame so when we manipulate the copy, we've still got our original data.
"""

# Make a copy
df_tmp = df.copy()

# Exactly same dataframe
df_tmp.saledate.head(20)

"""### Add datetime parameters for `saledate` column"""

df_tmp[:1].saledate.dt.year

df_tmp[:1].saledate.dt.day

df_tmp[:1].saledate

df_tmp["saleYear"] = df_tmp.saledate.dt.year
df_tmp["saleMonth"] = df_tmp.saledate.dt.month
df_tmp["saleDay"] = df_tmp.saledate.dt.day
df_tmp["saleDayOfWeek"] = df_tmp.saledate.dt.dayofweek
df_tmp["saleDayOfYear"] = df_tmp.saledate.dt.dayofyear

# Now we 've enriched our DataFrame with date time features, we can remove saledate
df_tmp.drop("saledate", axis=1, inplace=True)

# Check the values of different columns.
df_tmp.state.value_counts()

len(df_tmp)

"""## 5. Modelling

We've done enough enough EDA (we could always do more) but let's start to do some model-driven EDA.
"""

# Let's build a machine learning model
from sklearn.ensemble import RandomForestRegressor

model = RandomForestRegressor(n_jobs=-1, random_state=42)

# model.fit(df_tmp.drop("SalePrice", axis=1), df_tmp["SalePrice"])

"""### Convert String to categories

One way we can turn our data into numbers is by converting them into pandas categories.
"""

df_tmp.head().T

pd.api.types.is_string_dtype(df_tmp["UsageBand"])

# Find the colummns which contain strings
for label, content in df_tmp.items():
  if pd.api.types.is_string_dtype(content):
    print(label)

# If you're wondering what df.items() does, here's an example.
random_dict = {"key1": "hello", "key2": "world!"}

for key, value in random_dict.items():
  print(f"This is a key: {key}", f"This is a value: {value}")

# This will turn all of the string value into category values
for label, content in df_tmp.items():
  if pd.api.types.is_string_dtype(content):
    df_tmp[label] = content.astype("category").cat.as_ordered()

df_tmp.info()

df_tmp.state.cat.categories

df_tmp.state.value_counts()

df_tmp.state.cat.codes

"""Thanks to pandas Categories we now have a way to access all of our data in the form of numbers.

But we still have a bunch of missing data...

### Save preprocessed data
"""

# Export current tmp dataframe
df_tmp.to_csv("/train_tmp.csv", index=False)

# Import preprocessed data
df_tmp = pd.read_csv("/train_tmp.csv", low_memory=False)

df_tmp.head().T

df_tmp.isna().sum()

"""### Fill Missing Values

#### Fill Numerical missing values first
"""

for label, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    print(label)

df_tmp.ModelID

# Check for which numeric columns have null values
for label, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    if pd.isnull(content).sum():
      print(label)

# Fill numeric rows with the median
for label, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    if pd.isnull(content).sum():
      # Add a binary column which tells us that if the data is missing or not
      df_tmp[label+"_is_missing"] = pd.isnull(content)
      # Fill missing numeric values with median
      df_tmp[label] = content.fillna(content.median())

# Demonstrate how median is more robust than mean
hundreds = np.full((1000,), 100)
hundreds_billion = np.append(hundreds, 1000000000)
np.mean(hundreds), np.mean(hundreds_billion), np.median(hundreds), np.median(hundreds_billion)

hundreds

hundreds_billion

# Check if there's any null numeric values
for label, content in df_tmp.items():
  if pd.api.types.is_numeric_dtype(content):
    if pd.isnull(content).sum():
      print(label)

# Check to see how many examples were missing
df_tmp.auctioneerID.value_counts()

df_tmp.isna().sum()

"""### Filling and turning categorical variables into numbers"""

# Check for columns which aren't numeric
for label, content in df_tmp.items():
  if not pd.api.types.is_numeric_dtype(content):
    print(label)

pd.Categorical(df_tmp["state"]).dtype

# Turn Categorical variables into numbers and fill missing
for label, content in df_tmp.items():
  if not pd.api.types.is_numeric_dtype(content):
    # Add binary column to indicate whether sample had missing value
    df_tmp[label+"_is_missing"] = pd.isnull(content)
    # Turn categories into numbers and add +1
    df_tmp[label] = pd.Categorical(content).codes+1

df_tmp.info()

df_tmp.head().T

df_tmp.isna().sum()[:20]

"""Now that all of data is numeric as well as our dataframe has no missing values, we should be able to build a machine learning model."""

df_tmp.head()

len(df_tmp)

# Commented out IPython magic to ensure Python compatibility.
# %%time
# # Instantiate model
# model = RandomForestRegressor(n_jobs=-1, random_state=42)
# 
# # Fit the model
# model.fit(df_tmp.drop("SalePrice", axis=1), df_tmp["SalePrice"])

# Score the model
model.score(df_tmp.drop("SalePrice", axis=1), df_tmp["SalePrice"])

"""### Splitting data into train/validation sets"""

df_tmp.head()

df_tmp.saleYear.value_counts()

# Split data into training and validation
df_val = df_tmp[df_tmp.saleYear==2012]
df_train = df_tmp[df_tmp.saleYear != 2012]

len(df_val), len(df_train)

# Split data into X & y
X_train, y_train = df_train.drop("SalePrice", axis=1), df_train.SalePrice
X_valid, y_valid = df_val.drop("SalePrice", axis=1), df_val.SalePrice

X_train.shape, y_train.shape, X_valid.shape, y_valid.shape

y_train

y_train

"""### Building an evaluation function"""

# Create evaluation function (the competition uses RMSLE)
from sklearn.metrics import mean_squared_log_error, mean_absolute_error, r2_score

def rmsle(y_test, y_preds):
    """
    Caculates root mean squared log error between predictions and
    true labels.
    """
    return np.sqrt(mean_squared_log_error(y_test, y_preds))

# Create function to evaluate model on a few different levels
def show_scores(model):
    train_preds = model.predict(X_train)
    val_preds = model.predict(X_valid)
    scores = {"Training MAE": mean_absolute_error(y_train, train_preds),
              "Valid MAE": mean_absolute_error(y_valid, val_preds),
              "Training RMSLE": rmsle(y_train, train_preds),
              "Valid RMSLE": rmsle(y_valid, val_preds),
              "Training R^2": r2_score(y_train, train_preds),
              "Valid R^2": r2_score(y_valid, val_preds)}
    return scores

"""## Testing our model on a subset (to tune the hyperparameters)"""

# This takes far too long... for experimenting

# %%time
# model = RandomForestRegressor(n_jobs=-1,
#                               random_state=42)

# model.fit(X_train, y_train)

len(X_train)

# Change max_samples value
model = RandomForestRegressor(n_jobs=-1,
                              random_state=42,
                              max_samples=10000)

# %%time
# Cutting down on the max number of samples each estimator can see improves training time
model.fit(X_train, y_train)

(X_train.shape[0] * 100) / 1000000

10000 * 100

"""### Hyerparameter tuning with RandomizedSearchCV"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# from sklearn.model_selection import RandomizedSearchCV
# 
# # Different RandomForestRegressor hyperparameters
# rf_grid = {"n_estimators": np.arange(10, 100, 10),
#            "max_depth": [None, 3, 5, 10],
#            "min_samples_split": np.arange(2, 20, 2),
#            "min_samples_leaf": np.arange(1, 20, 2),
#            "max_features": [0.5, 1, "sqrt", "auto"],
#            "max_samples": [10000]}
# 
# # Instantiate RandomizedSearchCV model
# rs_model = RandomizedSearchCV(RandomForestRegressor(n_jobs=-1,
#                                                     random_state=42),
#                               param_distributions=rf_grid,
#                               n_iter=2,
#                               cv=5,
#                               verbose=True)
# 
# # Fit the RandomizedSearchCV model
# rs_model.fit(X_train, y_train)

# Find the best model hyperparameters
rs_model.best_params_

"""### Train a model with the best hyperparamters
**Note**: These were found after 100 iterations of `RandomizedSearchCV`
"""

# Commented out IPython magic to ensure Python compatibility.
# %%time
# 
# # Most ideal hyperparamters
# ideal_model = RandomForestRegressor(n_estimators=40,
#                                     min_samples_leaf=1,
#                                     min_samples_split=14,
#                                     max_features=0.5,
#                                     n_jobs=-1,
#                                     max_samples=None,
#                                     random_state=42) # random state so our results are reproducible
# 
# # Fit the ideal model
# ideal_model.fit(X_train, y_train)

ideal_model

"""### Make predictions on test data"""

# Import the test data
df_test = pd.read_csv("/Test.csv",
                      low_memory=False,
                      parse_dates=["saledate"])

df_test.head()

"""### Preprocessing the data (getting the test dataset in the same format as our training dataset)"""

def preprocess_data(df):
    """
    Performs transformations on df and returns transformed df.
    """
    df["saleYear"] = df.saledate.dt.year
    df["saleMonth"] = df.saledate.dt.month
    df["saleDay"] = df.saledate.dt.day
    df["saleDayOfWeek"] = df.saledate.dt.dayofweek
    df["saleDayOfYear"] = df.saledate.dt.dayofyear

    df.drop("saledate", axis=1, inplace=True)

    # Fill the numeric rows with median
    for label, content in df.items():
        if pd.api.types.is_numeric_dtype(content):
            if pd.isnull(content).sum():
                # Add a binary column which tells us if the data was missing or not
                df[label+"_is_missing"] = pd.isnull(content)
                # Fill missing numeric values with median
                df[label] = content.fillna(content.median())

        # Filled categorical missing data and turn categories into numbers
        if not pd.api.types.is_numeric_dtype(content):
            df[label+"_is_missing"] = pd.isnull(content)
            # We add +1 to the category code because pandas encodes missing categories as -1
            df[label] = pd.Categorical(content).codes+1

    return df

# Process the test data
df_test = preprocess_data(df_test)
df_test.head()

X_train.head()

# We can find how the columns differ using sets
set(X_train.columns) - set(df_test.columns)

# Manually adjust df_test to have auctioneerID_is_missing column
df_test["auctioneerID_is_missing"] = False
df_test.head()

"""We've made some predictions but they're not in the same format Kaggle is asking for: https://www.kaggle.com/c/bluebook-for-bulldozers/overview/evaluation"""

# Export prediction data
df_preds.to_csv("/test_predictions.csv", index=False)

"""## Feature Importance
Feature importance seeks to figure out which different attributes of the data were most importance when it comes to predicting the target variable (SalePrice).
"""

# Find feature importance of our best model
ideal_model.feature_importances_

# Helper function for plotting feature importance
def plot_features(columns, importances, n=20):
    df = (pd.DataFrame({"features": columns,
                        "feature_importances": importances})
          .sort_values("feature_importances", ascending=False)
          .reset_index(drop=True))

    # Plot the dataframe
    fig, ax = plt.subplots()
    ax.barh(df["features"][:n], df["feature_importances"][:20])
    ax.set_ylabel("Features")
    ax.set_xlabel("Feature importance")
    ax.invert_yaxis()

plot_features(X_train.columns, ideal_model.feature_importances_)

df["Enclosure"].value_counts()

